import requests
from bs4 import BeautifulSoup
import re
import time
import os
import random

def results_size_in_bytes(results):
    """ä¼°ç®—resultsåˆ—è¡¨å¤§å°ï¼Œç®€å•åœ°ç”¨å­—ç¬¦ä¸²é•¿åº¦æ±‚å’Œï¼Œutf-8ç¼–ç ä¸‹è¿‘ä¼¼"""
    size = 0
    for r in results:
        # æ‹¼æˆä¸€æ®µå­—ç¬¦ä¸²ï¼Œè®¡ç®—utf-8å­—èŠ‚é•¿åº¦
        s = (
            r['æ ‡é¢˜'] + r['æ—¥æœŸ'] + r['æè¿°'] + r['åœ°ç‚¹'] + r['é“¾æ¥'] + r['åŒ¹é…çš„æœç´¢å­—ç¬¦ä¸²'] + r['å®Œæ•´divå†…å®¹']
        )
        size += len(s.encode('utf-8'))
    return size

def save_results_to_txt(results, filename="matched_results.txt"):
    current_dir = os.path.dirname(os.path.abspath(__file__))
    filepath = os.path.join(current_dir, filename)

    with open(filepath, "a", encoding="utf-8") as f:  # è¿½åŠ å†™
        for r in results:
            f.write("ğŸ¯ åŒ¹é…ç»“æœ:\n")
            f.write(f"æ ‡é¢˜: {r['æ ‡é¢˜']}\n")
            f.write(f"æ—¥æœŸ: {r['æ—¥æœŸ']}\n")
            f.write(f"æè¿°: {r['æè¿°']}\n")
            f.write(f"åœ°ç‚¹: {r['åœ°ç‚¹']}\n")
            f.write(f"é“¾æ¥: {r['é“¾æ¥']}\n")
            f.write(f"åŒ¹é…å…³é”®è¯: {r['åŒ¹é…çš„æœç´¢å­—ç¬¦ä¸²']}\n")
            f.write("å®Œæ•´divå†…å®¹:\n")
            f.write(r["å®Œæ•´divå†…å®¹"] + "\n")
            f.write("-" * 80 + "\n")

    print(f"\nâœ… è¿½åŠ å†™å…¥ {len(results)} æ¡ç»“æœï¼Œæ–‡ä»¶ï¼š{filepath}")

def scrape_titles(base_url, search_strings, max_pages=500, start_page=1, max_buffer_size=1*1024*1024):
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'zh-CN,zh;q=0.9',
        'Referer': base_url,
        'Connection': 'keep-alive'
    }

    results_buffer = []
    visited_urls = set()
    next_url = f"{base_url}index.php?a=lists&cityid=11&page={start_page}"

    for page in range(start_page, start_page + max_pages):
        if not next_url or next_url in visited_urls:
            print("ğŸš« æ²¡æœ‰æ›´å¤šé¡µé¢æˆ–å·²è®¿é—®è¿‡")
            break
        visited_urls.add(next_url)

        try:
            response = requests.get(next_url, headers=headers, timeout=10)
            response.raise_for_status()
        except requests.RequestException as e:
            print(f"âš ï¸ ç¬¬ {page} é¡µè¯·æ±‚å¤±è´¥ï¼š{e}ï¼Œç­‰å¾…10ç§’åé‡è¯•ä¸€æ¬¡")
            time.sleep(10)
            try:
                response = requests.get(next_url, headers=headers, timeout=10)
                response.raise_for_status()
            except requests.RequestException as e:
                print(f"âŒ é‡è¯•åä¾ç„¶å¤±è´¥ï¼Œè·³è¿‡ç¬¬ {page} é¡µï¼š{e}")
                continue

        soup = BeautifulSoup(response.text, 'html.parser')

        info_items = soup.find_all('div', class_='InfoItem col-20')
        if not info_items:
            print(f"â›” ç¬¬ {page} é¡µæ²¡æœ‰ä»»ä½•æœ‰æ•ˆå†…å®¹ï¼Œåœæ­¢çˆ¬å–")
            break

        print(f"\nğŸ“„ æ­£åœ¨å¤„ç†ç¬¬ {page} é¡µï¼Œå…± {len(info_items)} ä¸ª div")

        for item in info_items:
            title_div = item.find('div', class_='Title')
            desc_div = item.find('div', class_='Desc')

            if not (title_div and desc_div):
                continue

            title = title_div.text.strip()
            desc = desc_div.text.strip()
            text_to_search = f"{title} {desc}"

            matched = None
            for search_string in search_strings:
                if search_string and re.search(search_string, text_to_search, re.IGNORECASE):
                    matched = search_string
                    break

            if not matched:
                continue

            state_div = item.find('div', class_='State')
            date_span = state_div.find('span').text.strip() if state_div and state_div.find('span') else 'æœªçŸ¥'

            location_div = item.find('div', class_='Location')
            location_span = location_div.find('span') if location_div else None
            location = location_span.text.strip() if location_span else 'æœªçŸ¥'

            link_tag = item.find('a')
            link = link_tag['href'] if link_tag and 'href' in link_tag.attrs else 'æ— é“¾æ¥'

            result = {
                'å®Œæ•´divå†…å®¹': str(item),
                'æ ‡é¢˜': title,
                'æ—¥æœŸ': date_span,
                'æè¿°': desc,
                'åœ°ç‚¹': location,
                'é“¾æ¥': f"{base_url}{link}" if link != 'æ— é“¾æ¥' else 'æ— é“¾æ¥',
                'åŒ¹é…çš„æœç´¢å­—ç¬¦ä¸²': matched
            }

            results_buffer.append(result)

            print(f"âœ… åŒ¹é…: {title} | å…³é”®è¯: {matched} | é“¾æ¥ :{base_url}{link}")

            # æ£€æŸ¥ç¼“å­˜å¤§å°ï¼Œè¾¾åˆ°é˜ˆå€¼åˆ™å†™å…¥æ–‡ä»¶ï¼Œæ¸…ç©ºç¼“å­˜
            if results_size_in_bytes(results_buffer) >= max_buffer_size:
                save_results_to_txt(results_buffer)
                results_buffer.clear()

        # æ‰¾ä¸‹ä¸€é¡µé“¾æ¥
        next_page_tag = soup.find('a', string=re.compile(r'ä¸‹ä¸€é¡µ'))
        if next_page_tag and next_page_tag.get('href'):
            next_href = next_page_tag.get('href')
            if not next_href.startswith("http"):
                next_url = base_url.rstrip('/') + '/' + next_href.lstrip('/')
            else:
                next_url = next_href
        else:
            print("ğŸš« æ²¡æœ‰æ‰¾åˆ°ä¸‹ä¸€é¡µé“¾æ¥ï¼Œåœæ­¢ç¿»é¡µ")
            break

        delay = random.randint(3, 10)
        print(f"â³ ç­‰å¾… {delay} ç§’åç»§ç»­...")
        time.sleep(delay)

    # çˆ¬å–ç»“æŸï¼Œå†™å…¥å‰©ä½™ç¼“å­˜æ•°æ®
    if results_buffer:
        save_results_to_txt(results_buffer)
        results_buffer.clear()

def main():
    base_url = "https://4.pin251.xyz/"

    search_strings = [

    ]

    start_page = 1  # èµ·å§‹é¡µ
    scrape_titles(base_url, search_strings, max_pages=550, start_page=start_page)

if __name__ == "__main__":
    main()
